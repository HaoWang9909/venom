{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/end2end/lib/python3.12/site-packages/torch/_utils.py:842: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/root/miniconda3/envs/end2end/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:366: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead\n",
      "  warnings.warn(\n",
      "/root/miniconda3/envs/end2end/lib/python3.12/inspect.py:586: UserWarning: The is_traceable field on torch.autograd.Function is deprecated and will be removed in PyTorch 2.4.\n",
      "  value = getter(object, key)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<script src=\"https://spcl.github.io/dace-webclient/dist/sdfv.js\"></script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import statistics\n",
    "import subprocess\n",
    "import ctypes\n",
    "\n",
    "import sten\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import timeit\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from grouped_nmv_tensor import SrNMTensor, nm_vector_mask_sparsify\n",
    "\n",
    "import spatha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.argv = ['your_notebook_name', '-m', '16', '-n', '2', '-v', '64']\n",
    "\n",
    "# 设置解析器\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('-m', type=int, default=16)\n",
    "parser.add_argument('-n', type=int, default=2)\n",
    "parser.add_argument('-v', type=int, default=128)\n",
    "\n",
    "parser.add_argument('--profile', action='store_true', default=False)\n",
    "parser.add_argument('--sparsetime', action='store_true', default=False)\n",
    "\n",
    "# 解析参数\n",
    "args = parser.parse_args()\n",
    "\n",
    "# 提取参数\n",
    "m          = args.m\n",
    "n          = args.n\n",
    "v          = args.v\n",
    "profile    = args.profile\n",
    "sparsetime = args.sparsetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def vnm_pruning_torch(matrix, V, N, M,):\n",
    "#     assert matrix.size(0) % V == 0, \"矩阵的行数需要是 V 的倍数\"\n",
    "#     assert matrix.size(1) % M == 0, \"矩阵的列数需要是 M 的倍数\"\n",
    "#     rows, cols = matrix.shape\n",
    "#     pruned_matrix = torch.zeros_like(matrix)\n",
    "\n",
    "#     # 遍历每个 VxM 块\n",
    "#     for row_block in range(0, rows, V):\n",
    "#         for col_block in range(0, cols, M):\n",
    "#             # 在每个块中随机选择 4 列作为可能的非零元素列\n",
    "#             possible_cols = torch.randperm(M)[:4] + col_block\n",
    "\n",
    "#             # 对每一行，从这 4 列中随机选择 N 列填入非零元素\n",
    "#             for v_row in range(V):\n",
    "#                 selected_cols = possible_cols[torch.randperm(4)[:N]]\n",
    "#                 pruned_matrix[row_block + v_row, selected_cols] = matrix[row_block + v_row, selected_cols]\n",
    "\n",
    "#     return pruned_matrix\n",
    "\n",
    "\n",
    "\n",
    "# # 随机初始化矩阵并转移到 GPU，使用半精度\n",
    "# matrix = torch.rand(4096, 2048, device=\"cuda:0\", dtype=torch.float16)\n",
    "\n",
    "# pruned_matrix = vnm_pruning_torch(matrix, v, n, m)\n",
    "\n",
    "# # 保存矩阵\n",
    "# torch.save(pruned_matrix, \"pruned_matrix.pt\")\n",
    "\n",
    "# 读取矩阵\n",
    "pruned_matrix = torch.load(\"pruned_matrix.pt\")\n",
    "pruned_matrix = pruned_matrix.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mask_and_columns(tensor, V, N, M):\n",
    "    rows, cols = tensor.shape\n",
    "    mask = torch.zeros_like(tensor, dtype=torch.int32)\n",
    "    columns = torch.zeros((rows // V, cols // M * 4), dtype=torch.int32)\n",
    "\n",
    "    # 使用unfold提取所有VxM块\n",
    "    blocks = tensor.unfold(0, V, V).unfold(1, M, M)\n",
    "\n",
    "    for i in range(blocks.shape[0]):\n",
    "        for j in range(blocks.shape[1]):\n",
    "            block = blocks[i, j]\n",
    "            nonzero_cols = block.nonzero(as_tuple=False)[:, 1]\n",
    "            unique_cols = torch.unique(nonzero_cols)\n",
    "            num_unique = min(len(unique_cols), 4)\n",
    "            columns[i, j*4: j*4 + num_unique] = unique_cols[:num_unique]\n",
    "\n",
    "            for idx in range(V):\n",
    "                row_nonzero_cols = block[idx, :].nonzero(as_tuple=False).squeeze()\n",
    "                mask[i*V + idx, j*M + row_nonzero_cols] = 1\n",
    "\n",
    "    return mask, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, columns = calculate_mask_and_columns(pruned_matrix, v, n, m)\n",
    "\n",
    "# 输出结果查看（可选，根据需要调整输出量）\n",
    "print(\"Mask:\")\n",
    "print(mask)\n",
    "print(\"Columns:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mask_and_columns(tensor, V, N, M):\n",
    "    rows, cols = tensor.shape\n",
    "    mask = torch.zeros_like(tensor, dtype=torch.int32)  # 使用 int32 类型\n",
    "    columns = torch.zeros((rows // V, cols // M * 4), dtype=torch.int32)  # 存储每个块中非零元素的列索引\n",
    "\n",
    "    # 遍历每个 VxM 的块\n",
    "    for row_block in range(0, rows, V):\n",
    "        for col_block in range(0, cols, M):\n",
    "            block = tensor[row_block:row_block + V, col_block:col_block + M]\n",
    "\n",
    "            # 确定块中所有非零元素的列位置\n",
    "            nonzero_cols = block.nonzero(as_tuple=False)[:, 1]  # 获取非零元素的列索引\n",
    "\n",
    "            if len(nonzero_cols) > 0:\n",
    "                # 从中提取前四个不重复的列索引\n",
    "                unique_cols = torch.unique(nonzero_cols)\n",
    "                num_unique = min(len(unique_cols), 4)  # 最多取四列\n",
    "                \n",
    "                # 更新 columns 矩阵\n",
    "                columns[row_block // V, (col_block // M) * 4: (col_block // M) * 4 + num_unique] = unique_cols[:num_unique]\n",
    "                \n",
    "                # 设置 mask\n",
    "                for idx in range(V):  # 对于块中的每一行\n",
    "                    row_nonzero_cols = block[idx, :].nonzero(as_tuple=False).squeeze()\n",
    "                    mask[row_block + idx, col_block + row_nonzero_cols] = 1\n",
    "\n",
    "    return mask, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask:\n",
      "tensor([[0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 1, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 1, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0', dtype=torch.int32)\n",
      "Columns:\n",
      "tensor([[ 1, 10, 14,  ...,  7,  8, 14],\n",
      "        [ 0,  3,  4,  ...,  3, 12, 13],\n",
      "        [ 0,  3, 11,  ...,  8, 11, 15],\n",
      "        ...,\n",
      "        [ 4,  5,  7,  ...,  2,  4, 11],\n",
      "        [ 0,  1,  4,  ...,  5, 10, 11],\n",
      "        [ 0,  5, 14,  ...,  8, 11, 12]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "mask, columns = calculate_mask_and_columns(pruned_matrix, v, n, m)\n",
    "\n",
    "# 输出结果查看（可选，根据需要调整输出量）\n",
    "print(\"Mask:\")\n",
    "print(mask)\n",
    "print(\"Columns:\")\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMVectorSparsifier:\n",
    "    def __init__(self, n, m, tileM, mask=None, columns=None):\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.tileM = tileM\n",
    "        self.mask = mask\n",
    "        self.columns = columns\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        if self.mask is None or self.columns is None:\n",
    "            # 未提供mask和columns，需要计算它们\n",
    "            mask, columns = calculate_mask_and_columns(tensor, self.tileM, self.n, self.m)\n",
    "\n",
    "        else:\n",
    "            # 使用提供的mask和columns\n",
    "            mask = self.mask\n",
    "            columns = self.columns\n",
    "        \n",
    "        # 使用sten库创建一个稀疏张量包装器，这个稀疏张量基于之前创建的mask和columns\n",
    "        sparse_mtx = sten.SparseTensorWrapper.wrapped_from_dense(\n",
    "            SrNMTensor(self.n, self.m, self.tileM, tensor, mask, columns, tensor.device),\n",
    "            tensor,\n",
    "            grad_fmt,\n",
    "        )\n",
    "        \n",
    "        return sparse_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "w = NMVectorSparsifier(n, m, v, mask, columns)(pruned_matrix).wrapped_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.4963, 0.0885, 0.2386,  ..., 0.6001, 0.4131, 0.2450], device='cuda:0',\n",
       "       dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values = torch.nn.Parameter(w.values)\n",
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2,  ..., 1, 2, 3],\n",
       "        [0, 1, 2,  ..., 1, 2, 3],\n",
       "        [0, 1, 2,  ..., 1, 2, 3],\n",
       "        ...,\n",
       "        [0, 1, 2,  ..., 1, 2, 3],\n",
       "        [0, 1, 2,  ..., 1, 2, 3],\n",
       "        [0, 1, 2,  ..., 1, 2, 3]], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = w.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_dense_mul_dispatch(sparse_values, sparse_indices, sparse_metadata, dense, nrows_sp, ncols_sp, ncols_d, m, n, v, nnz, bias):\n",
    "\n",
    "    dense_ = dense.contiguous()\n",
    "\n",
    "    output = spatha.spmm(sparse_metadata,  # metadata\n",
    "                          sparse_indices,   # indices\n",
    "                          sparse_values,    # values\n",
    "                          dense_,           # rhs_matrix\n",
    "                          bias,\n",
    "                          nrows_sp,         # A_num_rows\n",
    "                          ncols_sp,         # A_num_cols\n",
    "                          ncols_d,          # B_num_cols\n",
    "                          v,                # vec_length\n",
    "                          n,                # n\n",
    "                          m,                # m\n",
    "                          nnz,              # nnz\n",
    "                          0,                # seed\n",
    "                          32,               # mbrow\n",
    "                          4                 # brow\n",
    "                          )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrnmSpmm(torch.nn.Module):\n",
    "    def __init__(self, original: torch.nn.Linear):\n",
    "        super().__init__()\n",
    "        self.bias = original.bias\n",
    "\n",
    "        # Convert weights from original module to SrNM\n",
    "        w = NMVectorSparsifier(n, m, v)(original.weight).wrapped_tensor\n",
    "\n",
    "        self.values = torch.nn.Parameter(w.values)\n",
    "        #self.columns = self.register_buffer('columns', w.columns)\n",
    "        self.columns = w.columns\n",
    "        self.metadata = w.metadata\n",
    "\n",
    "        self.nrows_sp = w.nrows\n",
    "        self.ncols_sp = w.ncols\n",
    "        self.nnz      = w.nnz\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        flattened_input = torch.flatten(input, start_dim=0, end_dim=-2)\n",
    "\n",
    "        ncols_d  = flattened_input.T.shape[1]\n",
    "        DM, _    = flattened_input.shape\n",
    "\n",
    "        output = sparse_dense_mul_dispatch(self.values, self.columns, self.metadata, flattened_input.T, self.nrows_sp, self.ncols_sp,\n",
    "                                           ncols_d, m, n, v, self.nnz, self.bias)\n",
    "        output = output.reshape((*input.shape[0:-1], -1))[..., :DM]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_time(name, data, number):\n",
    "    for d in data:\n",
    "        time_ms = d / number * 1000\n",
    "        #print(f'n {n} m {m} format {name} time_ms {time_ms:.3f}')\n",
    "    ds = [(d / number * 1000) for d in data]\n",
    "    mean = statistics.mean(ds)\n",
    "    median = statistics.median(ds)\n",
    "    std = statistics.stdev(ds)\n",
    "\n",
    "    if name == \"n:m\":\n",
    "        cfg = str(n)+\",\"+str(m)+\",\"\n",
    "    else:\n",
    "        cfg = \"0,0,\"\n",
    "    print(\n",
    "        \"0,\"+cfg+str(v)+\",\"+str(mean)+\",\"+str(median)+\",\"+str(std)+\",\"+str(len(ds))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_to_spmm(mod, weights_to_sparsify):\n",
    "    if isinstance(mod, torch.nn.Linear):\n",
    "        return SrnmSpmm(mod)\n",
    "\n",
    "    for name, m in mod.named_children():\n",
    "        if isinstance(m, SrnmSpmm):\n",
    "            continue\n",
    "        if isinstance(m, torch.nn.Linear):\n",
    "            setattr(mod, name, SrnmSpmm(m))\n",
    "        elif m is not mod:\n",
    "            linear_to_spmm(m, weights_to_sparsify)\n",
    "\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder_layer_prototype(num_repeats, number):\n",
    "    # 加载原始的BERT大模型\n",
    "    model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-large-uncased')\n",
    "\n",
    "    # 加载第二个BERT大模型，并转移到CUDA设备上，使用半精度浮点数(half precision)进行计算以提高性能\n",
    "    model2 = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-large-uncased').to(device='cuda:0').half()\n",
    "\n",
    "    # 生成一个随机整数输入，大小为32x512，用于模型输入\n",
    "    input = torch.randint(low=0, high=100, size=(32, 512))\n",
    "\n",
    "    # 生成一个列表，包含模型中所有线性层（Linear layers）的权重，这些权重将被转换为稀疏格式\n",
    "    weights_to_sparsify = [\n",
    "        module\n",
    "        for module_name, module in model.named_modules()\n",
    "        if (\n",
    "            isinstance(module, torch.nn.modules.linear.Linear)\n",
    "            and \"encoder.layer\" in module_name\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # 把模型和输入都转移到CUDA设备上，并使用半精度浮点数进行处理\n",
    "    model = model.to(device='cuda:0').half()\n",
    "    input = input.to(device='cuda:0')\n",
    "\n",
    "    # 将选定的权重转换为稀疏格式，并创建一个新的稀疏模型\n",
    "    sparse_model = linear_to_spmm(model, weights_to_sparsify)\n",
    "\n",
    "    # 执行模型一次，通常用于预热缓存\n",
    "    output = sparse_model(input)\n",
    "\n",
    "    # 如果命令行参数指定进行性能分析\n",
    "    if args.profile:\n",
    "        # 开启一个性能分析会话，记录CPU和CUDA的活动\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                # 执行模型推理，记录相关数据\n",
    "                output = sparse_model(input)\n",
    "        # 导出分析数据\n",
    "        prof.export_stacks(\"/tmp/profiler_stacks.txt\", \"self_cuda_time_total\")\n",
    "        prof.export_chrome_trace(\"trace_sparse.json\")\n",
    "        print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
    "\n",
    "        # 重复上述过程，但是这次使用密集模型\n",
    "        with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True, with_stack=True) as prof:\n",
    "            with record_function(\"model_inference\"):\n",
    "                output = model2(input)\n",
    "        prof.export_stacks(\"/tmp/profiler_stacks_dense.txt\", \"self_cuda_time_total\")\n",
    "        prof.export_chrome_trace(\"trace_dense.json\")\n",
    "        print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=15))\n",
    "        exit()\n",
    "\n",
    "    # 使用timeit库，重复执行密集模型10次作为预热，然后进行正式的性能测试\n",
    "    timeit.repeat('output = model2(input)', repeat=10, number=number, globals=locals())\n",
    "    dense_times = timeit.repeat('output = model2(input)', repeat=num_repeats, number=number, globals=locals())\n",
    "    report_time('dense', dense_times, number)\n",
    "\n",
    "    # 对稀疏模型进行同样的预热和性能测试\n",
    "    timeit.repeat('output = sparse_model(input)', repeat=10, number=number, globals=locals())\n",
    "    sparse_times = timeit.repeat('output = sparse_model(input)', repeat=num_repeats, number=number, globals=locals())\n",
    "    report_time('n:m', sparse_times, number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb51f328500>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch. set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,0,0,64,28.492342207270365,28.310824069194496,0.7472337186168171,30\n",
      "0,2,8,64,56.45544687286019,56.43744091503322,0.9001977038644426,30\n"
     ]
    }
   ],
   "source": [
    "transformer_encoder_layer_prototype(num_repeats=30, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "end2end",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
